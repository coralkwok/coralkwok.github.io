---
layout: post
title:  <a href="https://mligg23.github.io/MO-VLN-Site/">MO-VLN:A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation</a>
date:   2023-06-18 00:00:00 +00:00
image: /project/movln/images/image.png
venue: <em> arxiv</em>, 2023.
categories: research
authors: <a href="https://scholar.google.com/citations?user=Iwj59kkAAAAJ">Xiwen Liang</a>*, <a href="https://github.com/mligg23">Liang Ma</a>*, <strong>Shanshan Guo</strong>, <a href="https://scholar.google.com/citations?hl=en&user=OEPMQEMAAAAJ">Jianhua Han</a>, <a href="https://xuhangcn.github.io/">Hang Xu</a>, <a href="https://www.semanticscholar.org/author/Shikui-Ma/1388448155">Shikui Ma</a>, <a href="https://lemondan.github.io/">Xiaodan Liang</a> (*equal contribution)
projectpage: https://mligg23.github.io/MO-VLN-Site/
code: https://github.com/liangcici/MO-VLN
video: https://www.youtube.com/watch?v=dJtuuctvw0s&t=48s
arxiv: https://arxiv.org/abs/2306.10322
---
In this paper, we introduce MO-VLN, a benchmark for multi-task visual language navigation in a 3D simulator developed using Unreal Engine 5 and rendered by real scenes. We utilize LLMs to construct diverse high-quality data of instruction type without human annotation. Our benchmark MO-VLN provides four zero-shot open-set tasks: 1) Target conditions navigation given a specific object category (e.g., "fork"); 2) Goal-conditional navigation given simple instructions (e.g., "Search and move toward the tennis ball"); 3) Following step-by-step instructions; 4) Finding abstract objects on a high-level basis Instructions (e.g., "I'm thirsty").